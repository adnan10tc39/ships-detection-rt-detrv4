flowchart TB
    %% ========== INPUT PREPARATION ==========
    subgraph Input["STEP 1: INPUT PREPARATION"]
        IMG["Input Image<br/>maritime_scene.jpg"]
        LOAD["Load Image<br/>(PIL/OpenCV)"]
        ORIG_SIZE["Original Size<br/>[W_orig, H_orig]<br/>e.g., [1920, 1080]"]
        PREPROC["Preprocessing<br/>- Resize to [640, 640]<br/>- Convert to Tensor<br/>- Normalize"]
        IMG_TENSOR["Image Tensor<br/>[1, 3, 640, 640]"]
        SIZE_TENSOR["Original Size Tensor<br/>[[1920, 1080]]"]
        
        IMG --> LOAD
        LOAD --> ORIG_SIZE
        ORIG_SIZE --> PREPROC
        PREPROC --> IMG_TENSOR
        PREPROC --> SIZE_TENSOR
    end
    
    %% ========== TEXT PROMPT PREPARATION ==========
    subgraph TextPrep["STEP 2: TEXT PROMPT PREPARATION"]
        PROMPTS["Text Prompts List<br/>['bulk cargo carrier',<br/>'container ship',<br/>'fishing boat',<br/>'general cargo ship',<br/>'ore carrier',<br/>'passenger ship']"]
        CLIP_ENC["CLIP Text Encoder<br/>(Frozen)<br/>openai/clip-vit-base-patch32"]
        TEXT_EMB["Text Embeddings<br/>[6, 512]<br/>One embedding per class<br/>Cached for efficiency"]
        
        PROMPTS --> CLIP_ENC
        CLIP_ENC --> TEXT_EMB
    end
    
    %% ========== VISUAL FEATURE EXTRACTION ==========
    subgraph Visual["STEP 3: VISUAL FEATURE EXTRACTION"]
        BACKBONE["HGNetv2 Backbone (B2)<br/>Multi-scale feature extraction"]
        F1["F1: [1, 384, 80, 80]<br/>stride 8"]
        F2["F2: [1, 768, 40, 40]<br/>stride 16"]
        F3["F3: [1, 1536, 20, 20]<br/>stride 32"]
        HYBRID_ENC["Hybrid Encoder<br/>- FPN (Feature Pyramid Network)<br/>- PAN (Path Aggregation Network)<br/>- AIFI (Attention-based Feature)"]
        ENH_FEAT["Enhanced Features<br/>[1, 256, H', W']<br/>Multi-scale feature fusion"]
        
        BACKBONE --> F1
        BACKBONE --> F2
        BACKBONE --> F3
        F1 --> HYBRID_ENC
        F2 --> HYBRID_ENC
        F3 --> HYBRID_ENC
        HYBRID_ENC --> ENH_FEAT
    end
    
    %% ========== OBJECT QUERY GENERATION ==========
    subgraph QueryGen["STEP 4: OBJECT QUERY GENERATION"]
        DFINE_DEC["DFINE Transformer Decoder<br/>- 300 Object Queries<br/>- 4 Decoder Layers"]
        VIS_EMB["Visual Embeddings<br/>[1, 300, 512]<br/>One per query"]
        PRED_BOXES["Predicted Boxes<br/>[1, 300, 4]<br/>Normalized (cxcywh format)"]
        
        DFINE_DEC --> VIS_EMB
        DFINE_DEC --> PRED_BOXES
    end
    
    %% ========== SIMILARITY COMPUTATION ==========
    subgraph Similarity["STEP 5: SIMILARITY COMPUTATION"]
        NORM_VIS["Normalize Visual Embeddings<br/>L2 normalization"]
        NORM_TEXT["Normalize Text Embeddings<br/>L2 normalization"]
        COS_SIM["Compute Cosine Similarity<br/>logits = visual @ text.T"]
        SIM_MAT["Similarity Matrix<br/>[1, 300, 6]<br/>Each query × Each class"]
        LOGIT_SCALE["Apply Logit Scale<br/>logits = logits * logit_scale<br/>(Learnable parameter)"]
        CLS_SCORES["Classification Scores<br/>[1, 300, 6]"]
        
        NORM_VIS --> COS_SIM
        NORM_TEXT --> COS_SIM
        COS_SIM --> SIM_MAT
        SIM_MAT --> LOGIT_SCALE
        LOGIT_SCALE --> CLS_SCORES
    end
    
    %% ========== CLASS ASSIGNMENT ==========
    subgraph ClassAssign["STEP 6: CLASS ASSIGNMENT"]
        SIGMOID["Apply Sigmoid<br/>scores = sigmoid(logits)"]
        FLATTEN["Flatten Scores<br/>[1, 1800] (300 × 6)"]
        TOPK["Select Top-K Queries<br/>Top 300 highest scores"]
        EXTRACT["Extract:<br/>- Query Index (0-299)<br/>- Class Index (0-5)<br/>- Score (confidence)"]
        RESULT["Result Examples:<br/>Query 5 → Class 2 (fishing boat), Score: 0.92<br/>Query 12 → Class 2 (fishing boat), Score: 0.88<br/>Query 45 → Class 1 (container ship), Score: 0.90<br/>... (up to 300 detections)"]
        
        SIGMOID --> FLATTEN
        FLATTEN --> TOPK
        TOPK --> EXTRACT
        EXTRACT --> RESULT
    end
    
    %% ========== BOUNDING BOX POST-PROCESSING ==========
    subgraph BoxProc["STEP 7: BOUNDING BOX POST-PROCESSING"]
        GATHER["Gather Selected Boxes<br/>boxes = boxes[selected_indices]"]
        CONVERT["Convert Format<br/>From: cxcywh (center, width, height)<br/>To: xyxy (x1, y1, x2, y2)"]
        SCALE["Scale to Original Image Size<br/>boxes *= [W_orig, H_orig, W_orig, H_orig]"]
        FINAL_BOXES["Final Boxes<br/>[N, 4] (N ≤ 300)<br/>Absolute coordinates<br/>Original image resolution"]
        
        GATHER --> CONVERT
        CONVERT --> SCALE
        SCALE --> FINAL_BOXES
    end
    
    %% ========== FILTERING AND NMS ==========
    subgraph Filtering["STEP 8: FILTERING AND NMS (Optional)"]
        DETECTIONS["Detections:<br/>- Labels: [N] (class IDs)<br/>- Boxes: [N, 4] (xyxy format)<br/>- Scores: [N] (confidence scores)"]
        CONF_THRESH["Apply Confidence Threshold<br/>Keep detections with score > threshold<br/>(e.g., 0.4)"]
        NMS["Non-Maximum Suppression (NMS)<br/>- Remove overlapping boxes<br/>- Keep highest scoring detection<br/>- IoU threshold: 0.5 (default)"]
        FILTERED["Filtered Detections<br/>[M] (M ≤ N ≤ 300)"]
        
        DETECTIONS --> CONF_THRESH
        CONF_THRESH --> NMS
        NMS --> FILTERED
    end
    
    %% ========== OUTPUT FORMAT ==========
    subgraph Output["STEP 9: OUTPUT FORMAT"]
        OUTPUT_DICT["Final Detections Dictionary:<br/>{<br/>  'labels': [2, 2, 2, 1, 1, 0, ...],<br/>  'boxes': [[x1,y1,x2,y2], ...],<br/>  'scores': [0.92, 0.88, 0.85, ...]<br/>}"]
        MAP_NAMES["Map Class IDs to Names:<br/>labels_names = [<br/>  'fishing boat',      # ID 2<br/>  'fishing boat',      # ID 2<br/>  'container ship',   # ID 1<br/>  'bulk cargo carrier' # ID 0<br/>]"]
        
        OUTPUT_DICT --> MAP_NAMES
    end
    
    %% ========== VISUALIZATION ==========
    subgraph Viz["STEP 10: VISUALIZATION"]
        DRAW["Draw Bounding Boxes:<br/>- For each detection<br/>- Draw rectangle on image<br/>- Add label text + score"]
        OUT_IMG["Output Image: result.jpg<br/>- Original image<br/>- Bounding boxes<br/>- Class labels<br/>- Confidence scores"]
        
        DRAW --> OUT_IMG
    end
    
    %% ========== FLOW CONNECTIONS ==========
    IMG_TENSOR --> BACKBONE
    SIZE_TENSOR --> SCALE
    TEXT_EMB --> NORM_TEXT
    ENH_FEAT --> DFINE_DEC
    VIS_EMB --> NORM_VIS
    PRED_BOXES --> GATHER
    CLS_SCORES --> SIGMOID
    RESULT --> GATHER
    FINAL_BOXES --> DETECTIONS
    FILTERED --> OUTPUT_DICT
    MAP_NAMES --> DRAW
    
    %% Styling
    classDef inputStyle fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    classDef textStyle fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px
    classDef visualStyle fill:#bbdefb,stroke:#1565c0,stroke-width:2px
    classDef similarityStyle fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    classDef outputStyle fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    classDef processStyle fill:#ffccbc,stroke:#d84315,stroke-width:2px
    
    class IMG,LOAD,ORIG_SIZE,PREPROC,IMG_TENSOR,SIZE_TENSOR inputStyle
    class PROMPTS,CLIP_ENC,TEXT_EMB,NORM_TEXT textStyle
    class BACKBONE,F1,F2,F3,HYBRID_ENC,ENH_FEAT,DFINE_DEC,VIS_EMB,PRED_BOXES visualStyle
    class NORM_VIS,COS_SIM,SIM_MAT,LOGIT_SCALE,CLS_SCORES,SIGMOID,FLATTEN,TOPK,EXTRACT similarityStyle
    class OUTPUT_DICT,MAP_NAMES,DRAW,OUT_IMG outputStyle
    class GATHER,CONVERT,SCALE,FINAL_BOXES,DETECTIONS,CONF_THRESH,NMS,FILTERED,RESULT processStyle

